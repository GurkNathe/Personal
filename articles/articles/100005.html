<h1>Autonomous Cars – A Solution to the Trolley Problem (Opinion)</h1>
<h4>By Ethan Krug</h4>
<hr>
<h2>Introduction</h2>
<img 
alt="File:Trolley Problem.svg" 
src="//upload.wikimedia.org/wikipedia/commons/thumb/f/fd/Trolley_Problem.svg/800px-Trolley_Problem.svg.png" 
decoding="async" 
srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/fd/Trolley_Problem.svg/1200px-Trolley_Problem.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fd/Trolley_Problem.svg/1600px-Trolley_Problem.svg.png 2x" 
style="max-width: 100%; float: right; margin-bottom: 10px;"
/>
<p>
    One of the most famous moral dilemmas in Philosophy is the Trolley Problem. 
    The Trolley Problem describes a scenario where a run-away trolley is speeding 
    down a track, and up ahead is a fork in the track. On the track the trolley 
    is headed, there are five people that can't get out of the way in time to 
    dodge the trolley, and on the other track is one person that won't be able to 
    get out of the way in time if the trolley were to be heading down said track. 
    You are beside the tracks and in front of a lever, said lever will switch the 
    track from the five-person track to the one-person track. The dilemma is 
    whether you pull the lever, saving the five people, but killing the one 
    person, or not pulling the lever saving one person, but killing five people.
</p>
<p>
    From a Utilitarian approach, you would always pull the lever, regardless of 
    variation, since one is less than five. However, people don't generally make 
    decisions like that. For example, if that one person was the person you love 
    most – parent, partner, friend, etc. – and the five people on the track are 
    people you either hate or hold apathy towards, I believe many people would 
    choose to not pull the lever and let the five people die.
</p>
<p>
    In the YouTube series “Mind Field”, by Vsauce, an educational YouTube channel, 
    they performed a Trolley Problem experiment where test subjects were unaware 
    of the situation, and had to actually live out the Trolley Problem, as 
    described above, except they weren't out near the tracks, but in a fake control 
    station for train tracks. In the video, very few participants switched the track. 
    A common response as to why they didn't switch the track was simply that they 
    froze up, and many of the participants tried to look for people to handle the 
    situation for them.
</p>
<p>
    Even though the sample size of the Mind Field experiment was small, it still 
    demonstrates the issue with this dilemma and applying various schools of thought 
    to it. People don't always act according to a narrow or strict set of ethics/morals, 
    like Utilitarianism presupposes. There are times when we freeze up, make the 
    wrong decision and regret it, or simply act in our own best interest. Whatever 
    the reason may be, depending on the details of the situation, we would react 
    differently.
</p>
<p>
    That's where the debate around autonomous cars (ACs) fits in. A real-world 
    application of the trolley problem that will become an issue in the near 
    future is how ACs respond to imminent emergencies. 
</p>
<h2>ACs and the Trolley Problem</h2>
<img 
    alt="File:Moral Machine Screenshot.png" 
    src="//upload.wikimedia.org/wikipedia/commons/thumb/3/37/Moral_Machine_Screenshot.png/800px-Moral_Machine_Screenshot.png" 
    decoding="async"
    srcset="//upload.wikimedia.org/wikipedia/commons/thumb/3/37/Moral_Machine_Screenshot.png/1200px-Moral_Machine_Screenshot.png 1.5x, //upload.wikimedia.org/wikipedia/commons/3/37/Moral_Machine_Screenshot.png 2x"
    style="max-width: 100%; float: right; margin-bottom: 10px;"
/>
<p>
    Take the scenario where you are in an autonomous car (AC), and the brakes 
    suddenly fail. Up ahead are five people crossing in the crosswalk. You can't turn 
    right because there is a building there, and the left lane is blocked by a concrete 
    barrier that is there because of construction. If nothing is done, the car will 
    continue traveling straight and kill the five people in the crosswalk, whereas, 
    if the car is directed into the left lane, it will crash into the barrier, killing 
    you. In this scenario, how should the AC react?
</p>
<p>
    Well, from a Utilitarian approach, the car should smash into the barrier, since 
    five is greater than one, which would result in the greater amount of “happiness” 
    from the tragic incident. However, from a rational perspective, irregardless of 
    ideology, why should the car, that is supposed to transport me and keep me safe, 
    smash into the barrier killing the passenger inside? If this were the case in the 
    real-world, I would argue, virtually no one would use ACs, since in an emergency 
    scenario, the passenger is killed so long as the other option would take more lives.
</p>
<p>
    So, how should an AC react to extreme, potentially life-ending scenarios? I believe 
    the solution has to be judged at an individual level, and not defined by an ideology, 
    or company. What I mean by this is for personal use ACs, the user should decide 
    how the vehicle is to react. 
</p>
<p>
    To go into further detail about how this would work, we have to look at an experiment 
    run called the Moral Machine. The Moral Machine is a game-like format were users 
    are prompted to solve various Trolley Problems where the trolley is an AC with 
    potentially passengers, and there is some kind of obstacle ahead of the vehicle, 
    either pedestrians, or a road barrier, like the scenario at the beginning of this 
    section. There are also indicators for whether the pedestrians are allowed to be in 
    the crosswalk at that time. 
</p>
<p>
    The Moral Machine is an effective way to tell the car how it should react, or at 
    least a curated version of it for this specific use, I believe. Upon purchasing an AC, 
    a user would be prompted to solve a series of Moral Machine-like questions to tell 
    the car how it should react, instead of the manufacturer/seller telling your car how it 
    should behave. In my mind, this would provide the fairest solution to the AC Trolley 
    Problem, for personal vehicles.
</p>
<p>
    For commercial vehicles, it is a similar solution. For services like Uber, assuming they
    incorporate ACs, instead of the people that will be riding it, the company would be responsible 
    for completing these questions. When the customer of the company wants to use their AC 
    services, they would have to agree to the settings set by the company to user their 
    product, essentially an additional section to the Terms of Service. 
</p>
<p>
    If we wanted to go a step further with commercial uses of ACs, using global data gathered 
    from the Moral Machine experiment, it could be mandated that AC services set their 
    preferences according to said data to align with local/regional values. To me, this would 
    provide a fairer services to users, and more equitable due to serving the needs based on 
    locality.
</p>
<h2>Issues with this Solution</h2>
<p>
    Just like with everything, there is room for improvement. While I believe the use of Moral 
    Machine-like questions to program how an AC will react in emergency situations is a good 
    solution to the issue of how ACs should react, there are issues with it.
</p>
<p>
    First of all, Moral Machine is a “test”, if you will, that is disconnected from the actual 
    scenarios one would experience. To further elaborate, when answering these questions, 
    people can be apathetic towards these scenarios and answer them from a disconnected 
    viewpoint. Essentially, people have a harder time empathizing with the people/beings 
    portrayed in these questions. So, the answers generally have a certain bias (what that is, 
    I don't know). For example, I could image a few scenarios where people would answer with a 
    bias: to save the car every time, to target certain groups of people (the Moral Machine 
    had different groups of people, like rich, poor, criminal, young, old, etc.), to cause the 
    most deaths possible. So, while I think using Moral Machine-like tests to program how private 
    ACs should react, there is certainly room for error.
</p>
<p>
    Similarly, with commercial ACs, if we use the method of local data to program decisions, it 
    could introduce biases as well, along with other concerns. While, with a large enough sample 
    size, biases are usually weeded out when looking at the average, if the whole group is biased 
    then the data will be as well. This raises concerns regarding the solution I proposed for 
    commercial ACs, if the data is biased to an undesirable direction (e.g., always crash the car), 
    then it would be unreasonable to use said data to dictate how the cars are programmed. 
    Additionally, if the local biases aren't extreme, but significantly different from people 
    visiting an area, it would cause a conflict, since visitors are more likely to use car services 
    than locals. There is a solution I thought of that would cover both of these cases: profiles 
    for commercial ACs. There could be default profiles, or potentially a profile that the user 
    sets up on their personal device/account that decides how the car reacts, irregardless of local 
    data. This would solve the issue of local bias, and disjoint morals with visitors. This solution 
    may be even better than the one I proposed previously since it provides the same fairness as 
    private ACs, but it also has the same issues I previously mentioned my solution for private
    ACs posses.
</p>
<h2>Conclusions</h2>
<p>
    The solutions proposed here are my opinions on how we solve the issue of how ACs handle potentially 
    life-ending scenarios. I think it elucidates a solution that loosens the grip lawmakers/government 
    would have over this, and gives more power to the private sector/public. While there are issues 
    with my solution, I don't believe we will ever come to a conclusion that doesn't have the potential 
    for harm through one mean or another, and I think the solutions I proposed here minimize harm by a 
    great degree.
</p>
<h2>Sources</h2>
<ul>
    <li>
        <a href="https://www.youtube.com/watch?v=1sl5KJ69qiA" target="_blank" rel="norefferer">Vsauce</a>
    </li>
    <li>
        <a href="https://www.moralmachine.net/" target="_blank" rel="norefferer">Moral Machine</a>
    </li>
</ul>