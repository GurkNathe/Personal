{"ast":null,"code":"import { createError } from '../../errors.js';\nimport { replaceDiacritics } from './diacritics.js';\nimport { SPLITTERS, SUPPORTED_LANGUAGES } from './languages.js';\nimport { stemmer as english } from './english-stemmer.js';\nfunction normalizeToken(prop, token) {\n  var _this_stopWords;\n  const key = \"\".concat(this.language, \":\").concat(prop, \":\").concat(token);\n  if (this.normalizationCache.has(key)) {\n    return this.normalizationCache.get(key);\n  }\n  // Remove stopwords if enabled\n  if ((_this_stopWords = this.stopWords) === null || _this_stopWords === void 0 ? void 0 : _this_stopWords.includes(token)) {\n    this.normalizationCache.set(key, '');\n    return '';\n  }\n  // Apply stemming if enabled\n  if (this.stemmer && !this.stemmerSkipProperties.has(prop)) {\n    token = this.stemmer(token);\n  }\n  token = replaceDiacritics(token);\n  this.normalizationCache.set(key, token);\n  return token;\n}\n/* c8 ignore next 10 */\nfunction trim(text) {\n  while (text[text.length - 1] === '') {\n    text.pop();\n  }\n  while (text[0] === '') {\n    text.shift();\n  }\n  return text;\n}\nfunction tokenize(input, language, prop) {\n  if (language && language !== this.language) {\n    throw createError('LANGUAGE_NOT_SUPPORTED', language);\n  }\n  /* c8 ignore next 3 */\n  if (typeof input !== 'string') {\n    return [input];\n  }\n  let tokens;\n  if (prop && this.tokenizeSkipProperties.has(prop)) {\n    tokens = [this.normalizeToken.bind(this, prop !== null && prop !== void 0 ? prop : '')(input)];\n  } else {\n    const splitRule = SPLITTERS[this.language];\n    tokens = input.toLowerCase().split(splitRule).map(this.normalizeToken.bind(this, prop !== null && prop !== void 0 ? prop : '')).filter(Boolean);\n  }\n  const trimTokens = trim(tokens);\n  if (!this.allowDuplicates) {\n    return Array.from(new Set(trimTokens));\n  }\n  return trimTokens;\n}\nexport async function createTokenizer() {\n  let config = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : {};\n  if (!config.language) {\n    config.language = 'english';\n  } else if (!SUPPORTED_LANGUAGES.includes(config.language)) {\n    throw createError('LANGUAGE_NOT_SUPPORTED', config.language);\n  }\n  // Handle stemming - It is disabled by default\n  let stemmer;\n  if (config.stemming || config.stemmer && !('stemming' in config)) {\n    if (config.stemmer) {\n      if (typeof config.stemmer !== 'function') {\n        throw createError('INVALID_STEMMER_FUNCTION_TYPE');\n      }\n      stemmer = config.stemmer;\n    } else {\n      if (config.language === 'english') {\n        stemmer = english;\n      } else {\n        throw createError('MISSING_STEMMER', config.language);\n      }\n    }\n  }\n  // Handle stopwords\n  let stopWords;\n  if (config.stopWords !== false) {\n    stopWords = [];\n    if (Array.isArray(config.stopWords)) {\n      stopWords = config.stopWords;\n    } else if (typeof config.stopWords === 'function') {\n      stopWords = await config.stopWords(stopWords);\n    } else if (config.stopWords) {\n      throw createError('CUSTOM_STOP_WORDS_MUST_BE_FUNCTION_OR_ARRAY');\n    }\n    // Make sure stopWords is just an array of strings\n    if (!Array.isArray(stopWords)) {\n      throw createError('CUSTOM_STOP_WORDS_MUST_BE_FUNCTION_OR_ARRAY');\n    }\n    for (const s of stopWords) {\n      if (typeof s !== 'string') {\n        throw createError('CUSTOM_STOP_WORDS_MUST_BE_FUNCTION_OR_ARRAY');\n      }\n    }\n  }\n  // Create the tokenizer\n  const tokenizer = {\n    tokenize,\n    language: config.language,\n    stemmer,\n    stemmerSkipProperties: new Set(config.stemmerSkipProperties ? [config.stemmerSkipProperties].flat() : []),\n    tokenizeSkipProperties: new Set(config.tokenizeSkipProperties ? [config.tokenizeSkipProperties].flat() : []),\n    stopWords,\n    allowDuplicates: Boolean(config.allowDuplicates),\n    normalizeToken,\n    normalizationCache: new Map()\n  };\n  tokenizer.tokenize = tokenize.bind(tokenizer);\n  tokenizer.normalizeToken = normalizeToken;\n  return tokenizer;\n}","map":{"version":3,"names":["createError","replaceDiacritics","SPLITTERS","SUPPORTED_LANGUAGES","stemmer","english","normalizeToken","prop","token","_this_stopWords","key","concat","language","normalizationCache","has","get","stopWords","includes","set","stemmerSkipProperties","trim","text","length","pop","shift","tokenize","input","tokens","tokenizeSkipProperties","bind","splitRule","toLowerCase","split","map","filter","Boolean","trimTokens","allowDuplicates","Array","from","Set","createTokenizer","config","arguments","undefined","stemming","isArray","s","tokenizer","flat","Map"],"sources":["/home/krug/Coding/JavaScript/Websites/Personal/personal/node_modules/.pnpm/@orama+orama@1.2.11/node_modules/@orama/orama/src/components/tokenizer/index.ts"],"sourcesContent":["import { createError } from '../../errors.js'\nimport { Stemmer, Tokenizer, DefaultTokenizerConfig } from '../../types.js'\nimport { replaceDiacritics } from './diacritics.js'\nimport { Language, SPLITTERS, SUPPORTED_LANGUAGES } from './languages.js'\nimport { stemmer as english } from './english-stemmer.js'\n\ninterface DefaultTokenizer extends Tokenizer {\n  language: Language\n  stemmer?: Stemmer\n  tokenizeSkipProperties: Set<string>\n  stemmerSkipProperties: Set<string>\n  stopWords?: string[]\n  allowDuplicates: boolean\n  normalizationCache: Map<string, string>\n  normalizeToken(this: DefaultTokenizer, token: string, prop: string | undefined): string\n}\n\nfunction normalizeToken(this: DefaultTokenizer, prop: string, token: string): string {\n  const key = `${this.language}:${prop}:${token}`\n\n  if (this.normalizationCache.has(key)) {\n    return this.normalizationCache.get(key)!\n  }\n\n  // Remove stopwords if enabled\n  if (this.stopWords?.includes(token)) {\n    this.normalizationCache.set(key, '')\n    return ''\n  }\n\n  // Apply stemming if enabled\n  if (this.stemmer && !this.stemmerSkipProperties.has(prop)) {\n    token = this.stemmer(token)\n  }\n\n  token = replaceDiacritics(token)\n  this.normalizationCache.set(key, token)\n  return token\n}\n\n/* c8 ignore next 10 */\nfunction trim(text: string[]): string[] {\n  while (text[text.length - 1] === '') {\n    text.pop()\n  }\n  while (text[0] === '') {\n    text.shift()\n  }\n  return text\n}\n\nfunction tokenize(this: DefaultTokenizer, input: string, language?: string, prop?: string): string[] {\n  if (language && language !== this.language) {\n    throw createError('LANGUAGE_NOT_SUPPORTED', language)\n  }\n\n  /* c8 ignore next 3 */\n  if (typeof input !== 'string') {\n    return [input]\n  }\n\n  let tokens: string[]\n  if (prop && this.tokenizeSkipProperties.has(prop)) {\n    tokens = [this.normalizeToken.bind(this, prop ?? '')(input)]\n  } else {\n    const splitRule = SPLITTERS[this.language]\n    tokens = input\n      .toLowerCase()\n      .split(splitRule)\n      .map(this.normalizeToken.bind(this, prop ?? ''))\n      .filter(Boolean)\n  }\n\n  const trimTokens = trim(tokens)\n\n  if (!this.allowDuplicates) {\n    return Array.from(new Set(trimTokens))\n  }\n\n  return trimTokens\n}\n\nexport async function createTokenizer(config: DefaultTokenizerConfig = {}): Promise<DefaultTokenizer> {\n  if (!config.language) {\n    config.language = 'english'\n  } else if (!SUPPORTED_LANGUAGES.includes(config.language)) {\n    throw createError('LANGUAGE_NOT_SUPPORTED', config.language)\n  }\n\n  // Handle stemming - It is disabled by default\n  let stemmer: Stemmer | undefined\n\n  if (config.stemming || (config.stemmer && !('stemming' in config))) {\n    if (config.stemmer) {\n      if (typeof config.stemmer !== 'function') {\n        throw createError('INVALID_STEMMER_FUNCTION_TYPE')\n      }\n\n      stemmer = config.stemmer\n    } else {\n      if (config.language === 'english') {\n        stemmer = english\n      } else {\n        throw createError('MISSING_STEMMER', config.language)\n      }\n    }\n  }\n\n  // Handle stopwords\n  let stopWords: string[] | undefined\n\n  if (config.stopWords !== false) {\n    stopWords = []\n\n    if (Array.isArray(config.stopWords)) {\n      stopWords = config.stopWords\n    } else if (typeof config.stopWords === 'function') {\n      stopWords = await config.stopWords(stopWords)\n    } else if (config.stopWords) {\n      throw createError('CUSTOM_STOP_WORDS_MUST_BE_FUNCTION_OR_ARRAY')\n    }\n\n    // Make sure stopWords is just an array of strings\n    if (!Array.isArray(stopWords)) {\n      throw createError('CUSTOM_STOP_WORDS_MUST_BE_FUNCTION_OR_ARRAY')\n    }\n\n    for (const s of stopWords) {\n      if (typeof s !== 'string') {\n        throw createError('CUSTOM_STOP_WORDS_MUST_BE_FUNCTION_OR_ARRAY')\n      }\n    }\n  }\n\n  // Create the tokenizer\n  const tokenizer: DefaultTokenizer = {\n    tokenize,\n    language: config.language,\n    stemmer,\n    stemmerSkipProperties: new Set(config.stemmerSkipProperties ? [config.stemmerSkipProperties].flat() : []),\n    tokenizeSkipProperties: new Set(config.tokenizeSkipProperties ? [config.tokenizeSkipProperties].flat() : []),\n    stopWords,\n    allowDuplicates: Boolean(config.allowDuplicates),\n    normalizeToken,\n    normalizationCache: new Map(),\n  }\n\n  tokenizer.tokenize = tokenize.bind(tokenizer)\n  tokenizer.normalizeToken = normalizeToken\n\n  return tokenizer\n}\n"],"mappings":"AAAA,SAASA,WAAW,QAAQ;AAE5B,SAASC,iBAAiB,QAAQ;AAClC,SAAmBC,SAAS,EAAEC,mBAAmB,QAAQ;AACzD,SAASC,OAAA,IAAWC,OAAO,QAAQ;AAanC,SAASC,eAAuCC,IAAY,EAAEC,KAAa,EAAU;MAQ/EC,eAAA;EAPJ,MAAMC,GAAA,MAAAC,MAAA,CAAS,IAAI,CAACC,QAAQ,OAAAD,MAAA,CAAIJ,IAAA,OAAAI,MAAA,CAAQH,KAAA,CAAO;EAE/C,IAAI,IAAI,CAACK,kBAAkB,CAACC,GAAG,CAACJ,GAAA,GAAM;IACpC,OAAO,IAAI,CAACG,kBAAkB,CAACE,GAAG,CAACL,GAAA;EACrC;EAEA;EACA,IAAI,CAAAD,eAAA,OAAI,CAACO,SAAS,cAAdP,eAAA,uBAAAA,eAAA,CAAgBQ,QAAA,CAAST,KAAA,GAAQ;IACnC,IAAI,CAACK,kBAAkB,CAACK,GAAG,CAACR,GAAA,EAAK;IACjC,OAAO;EACT;EAEA;EACA,IAAI,IAAI,CAACN,OAAO,IAAI,CAAC,IAAI,CAACe,qBAAqB,CAACL,GAAG,CAACP,IAAA,GAAO;IACzDC,KAAA,GAAQ,IAAI,CAACJ,OAAO,CAACI,KAAA;EACvB;EAEAA,KAAA,GAAQP,iBAAA,CAAkBO,KAAA;EAC1B,IAAI,CAACK,kBAAkB,CAACK,GAAG,CAACR,GAAA,EAAKF,KAAA;EACjC,OAAOA,KAAA;AACT;AAEA;AACA,SAASY,KAAKC,IAAc,EAAY;EACtC,OAAOA,IAAI,CAACA,IAAA,CAAKC,MAAM,GAAG,EAAE,KAAK,IAAI;IACnCD,IAAA,CAAKE,GAAG;EACV;EACA,OAAOF,IAAI,CAAC,EAAE,KAAK,IAAI;IACrBA,IAAA,CAAKG,KAAK;EACZ;EACA,OAAOH,IAAA;AACT;AAEA,SAASI,SAAiCC,KAAa,EAAEd,QAAiB,EAAEL,IAAa,EAAY;EACnG,IAAIK,QAAA,IAAYA,QAAA,KAAa,IAAI,CAACA,QAAQ,EAAE;IAC1C,MAAMZ,WAAA,CAAY,0BAA0BY,QAAA;EAC9C;EAEA;EACA,IAAI,OAAOc,KAAA,KAAU,UAAU;IAC7B,OAAO,CAACA,KAAA,CAAM;EAChB;EAEA,IAAIC,MAAA;EACJ,IAAIpB,IAAA,IAAQ,IAAI,CAACqB,sBAAsB,CAACd,GAAG,CAACP,IAAA,GAAO;IACjDoB,MAAA,GAAS,CAAC,IAAI,CAACrB,cAAc,CAACuB,IAAI,CAAC,IAAI,EAAEtB,IAAA,aAAAA,IAAA,cAAAA,IAAA,GAAQ,IAAImB,KAAA,EAAO;EAC9D,OAAO;IACL,MAAMI,SAAA,GAAY5B,SAAS,CAAC,IAAI,CAACU,QAAQ,CAAC;IAC1Ce,MAAA,GAASD,KAAA,CACNK,WAAW,GACXC,KAAK,CAACF,SAAA,EACNG,GAAG,CAAC,IAAI,CAAC3B,cAAc,CAACuB,IAAI,CAAC,IAAI,EAAEtB,IAAA,aAAAA,IAAA,cAAAA,IAAA,GAAQ,KAC3C2B,MAAM,CAACC,OAAA;EACZ;EAEA,MAAMC,UAAA,GAAahB,IAAA,CAAKO,MAAA;EAExB,IAAI,CAAC,IAAI,CAACU,eAAe,EAAE;IACzB,OAAOC,KAAA,CAAMC,IAAI,CAAC,IAAIC,GAAA,CAAIJ,UAAA;EAC5B;EAEA,OAAOA,UAAA;AACT;AAEA,OAAO,eAAeK,gBAAA,EAAgF;EAAA,IAAhEC,MAAA,GAAAC,SAAA,CAAArB,MAAA,QAAAqB,SAAA,QAAAC,SAAA,GAAAD,SAAA,MAAiC,CAAC,CAAC;EACvE,IAAI,CAACD,MAAA,CAAO9B,QAAQ,EAAE;IACpB8B,MAAA,CAAO9B,QAAQ,GAAG;EACpB,OAAO,IAAI,CAACT,mBAAA,CAAoBc,QAAQ,CAACyB,MAAA,CAAO9B,QAAQ,GAAG;IACzD,MAAMZ,WAAA,CAAY,0BAA0B0C,MAAA,CAAO9B,QAAQ;EAC7D;EAEA;EACA,IAAIR,OAAA;EAEJ,IAAIsC,MAAA,CAAOG,QAAQ,IAAKH,MAAA,CAAOtC,OAAO,IAAI,EAAE,cAAcsC,MAAK,GAAK;IAClE,IAAIA,MAAA,CAAOtC,OAAO,EAAE;MAClB,IAAI,OAAOsC,MAAA,CAAOtC,OAAO,KAAK,YAAY;QACxC,MAAMJ,WAAA,CAAY;MACpB;MAEAI,OAAA,GAAUsC,MAAA,CAAOtC,OAAO;IAC1B,OAAO;MACL,IAAIsC,MAAA,CAAO9B,QAAQ,KAAK,WAAW;QACjCR,OAAA,GAAUC,OAAA;MACZ,OAAO;QACL,MAAML,WAAA,CAAY,mBAAmB0C,MAAA,CAAO9B,QAAQ;MACtD;IACF;EACF;EAEA;EACA,IAAII,SAAA;EAEJ,IAAI0B,MAAA,CAAO1B,SAAS,KAAK,KAAK,EAAE;IAC9BA,SAAA,GAAY,EAAE;IAEd,IAAIsB,KAAA,CAAMQ,OAAO,CAACJ,MAAA,CAAO1B,SAAS,GAAG;MACnCA,SAAA,GAAY0B,MAAA,CAAO1B,SAAS;IAC9B,OAAO,IAAI,OAAO0B,MAAA,CAAO1B,SAAS,KAAK,YAAY;MACjDA,SAAA,GAAY,MAAM0B,MAAA,CAAO1B,SAAS,CAACA,SAAA;IACrC,OAAO,IAAI0B,MAAA,CAAO1B,SAAS,EAAE;MAC3B,MAAMhB,WAAA,CAAY;IACpB;IAEA;IACA,IAAI,CAACsC,KAAA,CAAMQ,OAAO,CAAC9B,SAAA,GAAY;MAC7B,MAAMhB,WAAA,CAAY;IACpB;IAEA,KAAK,MAAM+C,CAAA,IAAK/B,SAAA,EAAW;MACzB,IAAI,OAAO+B,CAAA,KAAM,UAAU;QACzB,MAAM/C,WAAA,CAAY;MACpB;IACF;EACF;EAEA;EACA,MAAMgD,SAAA,GAA8B;IAClCvB,QAAA;IACAb,QAAA,EAAU8B,MAAA,CAAO9B,QAAQ;IACzBR,OAAA;IACAe,qBAAA,EAAuB,IAAIqB,GAAA,CAAIE,MAAA,CAAOvB,qBAAqB,GAAG,CAACuB,MAAA,CAAOvB,qBAAqB,CAAC,CAAC8B,IAAI,KAAK,EAAE;IACxGrB,sBAAA,EAAwB,IAAIY,GAAA,CAAIE,MAAA,CAAOd,sBAAsB,GAAG,CAACc,MAAA,CAAOd,sBAAsB,CAAC,CAACqB,IAAI,KAAK,EAAE;IAC3GjC,SAAA;IACAqB,eAAA,EAAiBF,OAAA,CAAQO,MAAA,CAAOL,eAAe;IAC/C/B,cAAA;IACAO,kBAAA,EAAoB,IAAIqC,GAAA;EAC1B;EAEAF,SAAA,CAAUvB,QAAQ,GAAGA,QAAA,CAASI,IAAI,CAACmB,SAAA;EACnCA,SAAA,CAAU1C,cAAc,GAAGA,cAAA;EAE3B,OAAO0C,SAAA;AACT"},"metadata":{},"sourceType":"module","externalDependencies":[]}